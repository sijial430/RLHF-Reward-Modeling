{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "## Standard libraries\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "from fastcore.all import *\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "# set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "## project specifics\n",
    "import murdo\n",
    "import transformers\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating Epinet Into Armo Regression\n",
    "> C'mon, c'mon and meet the epinets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArmoRM's regression layer is pretty simple: it's just a linear layer, with a bias term, optimized through Ridge regression. In other words, it's just performing a change of basis on the underlying last-layer embeddings of Llama3 8b. And apparently, dimensions already exist in this marvelous latent space that correspond to many of the dimensions we care about.\n",
    "\n",
    "What can the epinet add to this? First, the structure fo the epinet should be pretty simple: we'll replicate the linear layer being used with Ridge Regression an MLP (hoping gradient descent is sufficient to extract the same dimensional information; and assuming we need extra compute power to both get the dimensions and reason about their epistemic status), but will multiply the output by the random epistemic index. In theory, this should allow the epinet to reduce the MSE regression loss by adding randomness to dimensions with questionable epistemic status.\n",
    "\n",
    "In this notebook, we integrate an mlp epinet with the regression layer. We'll try the simplest possible integration first, then perform training (which, given the small size, should be pretty quick), and iterate.\n",
    "\n",
    "How will we measure whether the integration works? First, we can sanity check by seeing how uncertainty compares across dimensions of rewards, comparing with our prior data on which dimensions have the most activation and which appear to be duplicates of each other. Ultimately, we can measure the performance of the reward model by doing some uncertainty-weighted best of N search.\n",
    "\n",
    "A note on form: prior versions of zetteldev had an emphasis on atomic notebooks for experimentation. We break from that. This document is more 'computational essay/lab report' than slip. It will contain many ideas, and confront much computational and ideological reducibility. The metaphors worthy of further abstraction will be highlighted in a separate report, so see the 'Reports' folder for the high level summary. What follows is a 'lab report' in chronological order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis**:\n",
    "1. Integrating the MLP reward model with an MLP epinet will enable the prediction of uncertainty per reward dimension per prompt.\n",
    "2. The uncertainty estimate should change with prompt response pairs.\n",
    "3. When the gating layer denotes a reward dimension as irrelevant, it should have a higher uncertainty.\n",
    "4. Integrating uncertainty into a Best of N sampler from a base llama model should have superior performance to the reward model without uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machinery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll set up the pretrained reward model, then extract dimensions and such from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, dataset_name = (\"FsfairX-LLaMA3-RM-v0.1\", \"ArmoRM-Multi-Objective-Data-v0.1\")\n",
    "save_dir = os.path.join(\"/home/piriac\", \"data\", \"ArmoRM\", \"regression_weights\")\n",
    "save_path = os.path.join(save_dir, f\"{model_name}_{dataset_name}.pt\")\n",
    "regression_layer = torch.load(save_path)[\"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_attributes, hidden_size = regression_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_rewards = torch.rand(800,hidden_size) @ regression_layer.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset prompt-response embeddings from the base LLM, as inputs to the regression layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from safetensors.torch import load_file\n",
    "from argparse import ArgumentParser\n",
    "def load_embeddings_and_preferences(embeddings_dir=None, model_name=None, dataset_name=None):\n",
    "    \"\"\"\n",
    "    Load embeddings and preferences from safetensors files.\n",
    "\n",
    "    Args:\n",
    "        embeddings_dir (str, optional): Path to embeddings directory\n",
    "        model_name (str, optional): Name of the model\n",
    "        dataset_name (str, optional): Name of the dataset\n",
    "\n",
    "    Returns:\n",
    "        tuple: (embeddings tensor, labels tensor)\n",
    "    \"\"\"\n",
    "    # Set default paths if not provided\n",
    "    HOME = os.path.expanduser(\"~\")\n",
    "    if embeddings_dir is None:\n",
    "        embeddings_dir = os.path.join(\n",
    "            HOME, \"data\", \"ArmoRM\", \"embeddings\", model_name, dataset_name\n",
    "        )\n",
    "\n",
    "    # Collect all embedding files\n",
    "    embedding_files = sorted(glob(f\"{embeddings_dir}-*.safetensors\"))\n",
    "\n",
    "    if not embedding_files:\n",
    "        raise FileNotFoundError(f\"No embedding files found in {embeddings_dir}\")\n",
    "\n",
    "    # Initialize lists to store embeddings and labels\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    print(\"Loading embeddings and labels from Safetensors files...\")\n",
    "    for file in tqdm(embedding_files, desc=\"Loading embeddings\"):\n",
    "        # Load the safetensors file\n",
    "        data = load_file(file)\n",
    "        embeddings.append(data[\"embeddings\"])  # Append embeddings tensor\n",
    "        labels.append(data[\"labels\"])  # Append labels tensor\n",
    "\n",
    "    # Concatenate all embeddings and labels into single tensors\n",
    "    embeddings = torch.cat(embeddings, dim=0).float()\n",
    "    labels = torch.cat(labels, dim=0).float()\n",
    "\n",
    "    print(f\"Total embeddings loaded: {embeddings.shape[0]}\")\n",
    "    print(f\"Total labels loaded: {labels.shape[0]}\")\n",
    "\n",
    "    # Verify shapes match\n",
    "    assert embeddings.shape[0] == labels.shape[0], \"Number of embeddings and labels must match\"\n",
    "\n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings and labels from Safetensors files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading embeddings: 100%|██████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1269.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings loaded: 569185\n",
      "Total labels loaded: 569185\n"
     ]
    }
   ],
   "source": [
    "embeddings, sparse_rewards = load_embeddings_and_preferences(\n",
    "    model_name=\"FsfairX-LLaMA3-RM-v0.1\",\n",
    "    dataset_name=\"ArmoRM-Multi-Objective-Data-v0.1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 15% of the reward labels are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([569185, 19])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_rewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1647670])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_rewards[sparse_rewards == sparse_rewards].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15235727168532293"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1647670/(569185*19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4844,  1.1484,  1.0781,  ..., -0.5156, -0.5703,  1.9453],\n",
       "        [-1.4453,  1.3984,  0.5352,  ..., -0.4980, -0.8047,  1.6328],\n",
       "        [-1.5156,  1.7812,  1.5781,  ...,  0.1514, -0.6133,  2.3438],\n",
       "        ...,\n",
       "        [-1.4141,  1.5781,  1.1484,  ..., -0.9023, -0.9961,  1.7500],\n",
       "        [-1.3906,  1.7422,  1.2969,  ..., -0.6797, -0.4570,  2.0000],\n",
       "        [-1.4531,  1.8125,  1.3047,  ..., -0.9688, -0.3887,  2.2344]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check the regression weights by seeing how well it matches the preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_rewards = embeddings @ regression_layer.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~torch.isnan(sparse_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diff = (sparse_rewards - predicted_rewards)[~torch.isnan(sparse_rewards)].numpy()\n",
    "mse_diff = np.mean(diff**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022128979"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the reward prediction is extremely successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the epinet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first epinet, we'll use a two layer mlp for the randomized component, and a pure linear layer for deterministic component. Thus, the non-randomized network recreates the Ridge regression setting. The epinet is given slightly more structure under the intuition that it needs not only to reproduce the computations of the deterministic component, but also reason about when those calculations need added randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from murdo.epinet_mlp import make_mlp_epinet\n",
    "epinet, indexer = make_mlp_epinet(\n",
    "    output_sizes = [hidden_size,n_attributes],\n",
    "    epinet_hiddens = [hidden_size + n_attributes, 512],\n",
    "    index_dim = 8,\n",
    "    prior_scale = 1,\n",
    "    name = \"my first epinet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPEpinet(\n",
       "  (base_mlp): ExposedMLP(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=4096, out_features=19, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (train_epinet): ProjectedMLP(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=4115, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=152, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (prior_epinet): ProjectedMLP(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=4115, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=152, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epinet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage\n",
    "output = epinet(torch.randn(64, hidden_size), indexer(64))\n",
    "train_predictions = output.train\n",
    "prior_predictions = output.prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 19])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4685,  0.1797, -0.7740,  0.2451,  1.7227, -0.9314, -0.2282, -0.9275])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we'll follow the same principle as is the paper: simply masking the unknown dimensions when calculating losses. This is hopefully sufficiently in keeping with the nature of SGD. Future work might explore using uncertainty to more cleverly compensate for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "def train_epinet(epinet, indexer, embeddings, sparse_rewards, hidden_size,\n",
    "                 batch_size=64, num_epochs=100, lr=1e-3, load_latest=False):\n",
    "    \"\"\"\n",
    "    Train the epinet using masked MSE loss.\n",
    "\n",
    "    Args:\n",
    "        epinet: The epinet MLP model\n",
    "        indexer: The indexer function for epinet\n",
    "        embeddings: Input embeddings tensor\n",
    "        sparse_rewards: Sparse reward labels tensor\n",
    "        hidden_size: Size of the hidden dimension\n",
    "        batch_size: Batch size for training\n",
    "        num_epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        load_latest: If True, load most recent saved model if it exists\n",
    "    \"\"\"\n",
    "    # Setup save directory\n",
    "    save_dir = os.path.join(os.path.expanduser(\"~\"), \"data\", \"ArmoRM\", \"weights\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Check for latest saved model if requested\n",
    "    if load_latest:\n",
    "        model_files = glob.glob(os.path.join(save_dir, \"epinet_*.pt\"))\n",
    "        if model_files:\n",
    "            latest_model = max(model_files, key=os.path.getctime)\n",
    "            print(f\"Loading latest model from {latest_model}\")\n",
    "            epinet.load_state_dict(torch.load(latest_model))\n",
    "            return epinet\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    epinet = epinet.to(device)\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = optim.Adam(epinet.parameters(), lr=lr)\n",
    "\n",
    "    # Calculate number of batches\n",
    "    n_samples = embeddings.shape[0]\n",
    "    n_batches = n_samples // batch_size\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Shuffle data\n",
    "        perm = torch.randperm(n_samples)\n",
    "        embeddings = embeddings[perm]\n",
    "        sparse_rewards = sparse_rewards[perm]\n",
    "\n",
    "        # Batch training\n",
    "        pbar = tqdm(range(n_batches), desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for b in pbar:\n",
    "            # Get batch and move to device\n",
    "            start_idx = b * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_embeddings = embeddings[start_idx:end_idx].to(device)\n",
    "            batch_rewards = sparse_rewards[start_idx:end_idx].to(device)\n",
    "\n",
    "            # Generate random indices for epinet\n",
    "            indices = indexer(batch_size).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            epiout = epinet(batch_embeddings, indices)\n",
    "            predicted_rewards = epiout.train + epiout.prior # preweighted sum of the learnable and fixed components\n",
    "\n",
    "            # Create mask for non-nan values\n",
    "            mask = ~torch.isnan(batch_rewards)\n",
    "\n",
    "            # Calculate masked MSE loss\n",
    "            loss = torch.mean((predicted_rewards[mask] - batch_rewards[mask])**2)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': epoch_loss/(b+1)})\n",
    "\n",
    "            # Free memory\n",
    "            del batch_embeddings\n",
    "            del batch_rewards\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Print epoch statistics\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {epoch_loss/n_batches:.6f}')\n",
    "\n",
    "        # Save model with timestamp\n",
    "        if epoch % 25 == 0:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            save_path = os.path.join(save_dir, f\"epinet_{timestamp}.pt\")\n",
    "            torch.save(epinet.state_dict(), save_path)\n",
    "            print(f\"Model saved to {save_path}\")\n",
    "\n",
    "    return epinet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train! From above, Ridge regression without the epistemic nn achieved a loss of 0.022. Let's see if we can match that in the same order of magnitude, and perhaps even get below it.\n",
    "Although as we've also changed the methodology (to gradient descent), and are adding randomness to the outputs, the raw numbers aren't directly comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below, we can quickly get in the same order of magnitude; the remainder of training is how to exploit randomness efficiently to further minimize the loss. It will be a good sanity check to see how much randomness is added: is the plain MLP doing most of the work, or are outputs of a substantial magnitude coming from the randomized portion of the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading latest model from /home/piriac/data/ArmoRM/weights/epinet_20241111_163611.pt\n"
     ]
    }
   ],
   "source": [
    "trained_epinet = train_epinet(\n",
    "    epinet=epinet,\n",
    "    indexer=indexer,\n",
    "    embeddings=embeddings,\n",
    "    sparse_rewards=sparse_rewards,\n",
    "    hidden_size=hidden_size,\n",
    "    lr = 1e-5,\n",
    "    load_latest = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPEpinet(\n",
       "  (base_mlp): ExposedMLP(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=4096, out_features=19, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (train_epinet): ProjectedMLP(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=4115, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=152, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (prior_epinet): ProjectedMLP(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=4115, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=152, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_epinet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need in evaluating our epinet is a quantification of uncertainty per dimension. We can then perform this measurement across the dataset, and report:\n",
    "1. Average uncertainty per dimension across all samples\n",
    "2. Variance of uncertainty per dimension across all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_epinet_outputs(trained_epinet, indexer, embeddings, n_samples=10, batch_size=128):\n",
    "    \"\"\"\n",
    "    Sample multiple outputs from a trained epinet for each input embedding.\n",
    "\n",
    "    Args:\n",
    "        trained_epinet: The trained epinet model\n",
    "        indexer: The indexer function for generating random indices\n",
    "        embeddings: Input embeddings tensor of shape (n_embeddings, hidden_size)\n",
    "        n_samples: Number of samples to generate per input\n",
    "        batch_size: Batch size for processing\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Array of shape (n_embeddings, n_dimensions, n_samples)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    trained_epinet = trained_epinet.to(device)\n",
    "\n",
    "    n_embeddings = embeddings.shape[0]\n",
    "    n_dimensions = 19  # number of reward dimensions\n",
    "\n",
    "    # Initialize output tensor\n",
    "    all_outputs = torch.zeros((n_embeddings, n_dimensions, n_samples), device=device)\n",
    "    # Process in batches\n",
    "    with torch.no_grad():  # disable gradient computation for inference\n",
    "        for batch_start in tqdm(range(0, n_embeddings, batch_size), desc=\"Processing batches\"):\n",
    "            # Get batch of embeddings\n",
    "            batch_end = min(batch_start + batch_size, n_embeddings)\n",
    "            batch_embeddings = embeddings[batch_start:batch_end].to(device)\n",
    "\n",
    "            # Initialize batch predictions tensor\n",
    "            batch_predictions = torch.zeros((batch_end - batch_start, n_dimensions, n_samples), device=device)\n",
    "\n",
    "            # Sample multiple times for each embedding\n",
    "            for i in range(n_samples):\n",
    "                # Generate index for this sample\n",
    "                indices = indexer(1).to(device)\n",
    "\n",
    "                # Get predictions for this sample\n",
    "                outputs = trained_epinet(batch_embeddings, indices)\n",
    "                predictions = outputs.train + outputs.prior  # combine train and prior predictions\n",
    "\n",
    "                # Store predictions for this sample\n",
    "                batch_predictions[:, :, i] = predictions\n",
    "\n",
    "            # Store in output tensor\n",
    "            all_outputs[batch_start:batch_end] = batch_predictions\n",
    "            # Clear GPU memory\n",
    "            del batch_embeddings, indices, outputs, predictions, batch_predictions\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return all_outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0472, -0.2657, -0.1110,  0.4044, -0.4292,  0.1513,  1.2102,  0.9952],\n",
       "        [ 1.7883,  0.4186,  0.1676, -0.7698,  0.7433, -0.2450, -0.6685,  1.3462],\n",
       "        [ 1.4534, -0.7636,  1.0673,  1.5691,  0.1276,  0.7721, -0.9738,  0.2645],\n",
       "        [-1.1231, -0.3579, -0.8793, -0.5154, -1.2087,  0.3077,  0.3038,  0.5613],\n",
       "        [-1.2444,  2.1400, -0.8152,  0.1227,  0.9151, -1.0056, -0.0136,  1.8474],\n",
       "        [-0.2797,  0.8993, -0.6702,  0.1811,  1.3301,  2.0206,  0.1199,  0.3323],\n",
       "        [ 0.1806, -0.6630, -0.4456, -2.9613, -0.4385, -2.2631, -1.5685,  0.9717],\n",
       "        [ 0.1786, -0.2793,  0.4931,  0.2095,  0.7488,  0.1491,  0.5362,  0.8238],\n",
       "        [-2.1192, -0.1512, -0.5590, -0.7132,  0.2609, -1.0812, -0.1860, -1.0017],\n",
       "        [ 0.6723,  0.4385, -1.6923, -1.0989, -2.0868,  1.5404, -0.2603, -0.5551]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.vstack([indexer(42) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3999, -1.1252,  1.6562, -0.0490, -1.0774,  0.9167,  0.5866, -1.7774])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████████████████████████████████████████████████████████████| 4447/4447 [02:02<00:00, 36.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "samples = sample_epinet_outputs(\n",
    "    trained_epinet=trained_epinet,\n",
    "    indexer=indexer,\n",
    "    embeddings=embeddings,\n",
    "    n_samples=100,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "# Calculate statistics\n",
    "mean_predictions = samples.mean(dim=2)  # Average across samples\n",
    "std_predictions = samples.std(dim=2)    # Standard deviation across samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([569185, 19, 100])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8302, 0.8226, 0.8270, 0.8269, 0.8257, 0.8240, 0.8211, 0.8343, 0.8253,\n",
       "        0.8223, 0.8288, 0.8289, 0.8213, 0.8235, 0.8292, 0.8318, 0.8292, 0.8311,\n",
       "        0.8258, 0.8282, 0.8293, 0.8322, 0.8263, 0.8222, 0.8156, 0.8253, 0.8249,\n",
       "        0.8273, 0.8247, 0.8229, 0.8323, 0.8321, 0.8253, 0.8250, 0.8377, 0.8261,\n",
       "        0.8342, 0.8233, 0.8187, 0.8223, 0.8308, 0.8230, 0.8308, 0.8287, 0.8291,\n",
       "        0.8339, 0.8250, 0.8210, 0.8233, 0.8233, 0.8323, 0.8236, 0.8270, 0.8285,\n",
       "        0.8322, 0.8258, 0.8286, 0.8227, 0.8285, 0.8298, 0.8220, 0.8281, 0.8248,\n",
       "        0.8198, 0.8251, 0.8285, 0.8313, 0.8244, 0.8372, 0.8305, 0.8231, 0.8253,\n",
       "        0.8213, 0.8281, 0.8307, 0.8338, 0.8303, 0.8219, 0.8284, 0.8319, 0.8324,\n",
       "        0.8192, 0.8254, 0.8315, 0.8333, 0.8329, 0.8281, 0.8293, 0.8272, 0.8337,\n",
       "        0.8228, 0.8244, 0.8228, 0.8132, 0.8244, 0.8264, 0.8263, 0.8205, 0.8251,\n",
       "        0.8238], device='cuda:0')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0045, 0.0060, 0.0076,  ..., 0.0146, 0.0137, 0.0161],\n",
       "        [0.0029, 0.0037, 0.0054,  ..., 0.0105, 0.0087, 0.0120],\n",
       "        [0.0044, 0.0059, 0.0075,  ..., 0.0144, 0.0136, 0.0160],\n",
       "        ...,\n",
       "        [0.0037, 0.0046, 0.0054,  ..., 0.0126, 0.0094, 0.0140],\n",
       "        [0.0048, 0.0066, 0.0071,  ..., 0.0162, 0.0132, 0.0175],\n",
       "        [0.0036, 0.0044, 0.0052,  ..., 0.0122, 0.0090, 0.0136]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.std(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0045, 0.0051, 0.0063, 0.0040, 0.0060, 0.0023, 0.0023, 0.0032, 0.0026,\n",
       "        0.0018, 0.0058, 0.0012, 0.0061, 0.0119, 0.0164, 0.0163, 0.0142, 0.0129,\n",
       "        0.0151], device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_std_per_dimension = (samples.std(dim=2)).mean(dim=0)  # Average across samples\n",
    "mean_std_per_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0033, 0.0038, 0.0065, 0.0050, 0.0035, 0.0045, 0.0037, 0.0034, 0.0032,\n",
       "        0.0029, 0.0039, 0.0026, 0.0015, 0.0042, 0.0052, 0.0043, 0.0039, 0.0045,\n",
       "        0.0049], device='cuda:0')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_of_std_per_dimension = (samples.std(dim=2)).std(dim=0)  # Average across samples\n",
    "std_of_std_per_dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These early results show that \n",
    "1. different reward dimensions have different 'uncertainties'.\n",
    "2. The uncertainty per reward dimension changes across samples on a magnitude equal to the original std.\n",
    "3. The last six dimensions are, weirdly, far more uncertain than the first 13. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rmurdo-zetteldev",
   "language": "python",
   "name": "rmurdo-zetteldev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

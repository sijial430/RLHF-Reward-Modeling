{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "## Standard libraries\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "from fastcore.all import *\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "# set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "## project specifics\n",
    "import murdo\n",
    "import transformers\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating Epinet Into Armo Regression\n",
    "> C'mon, c'mon and meet the epinets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArmoRM's regression layer is pretty simple: it's just a linear layer, with a bias term, optimized through Ridge regression. In other words, it's just performing a change of basis on the underlying last-layer embeddings of Llama3 8b. And apparently, dimensions already exist in this marvelous latent space that correspond to many of the dimensions we care about.\n",
    "\n",
    "What can the epinet add to this? First, the structure fo the epinet should be pretty simple: we'll replicate the linear layer being used with Ridge Regression an MLP (hoping gradient descent is sufficient to extract the same dimensional information; and assuming we need extra compute power to both get the dimensions and reason about their epistemic status), but will multiply the output by the random epistemic index. In theory, this should allow the epinet to reduce the MSE regression loss by adding randomness to dimensions with questionable epistemic status.\n",
    "\n",
    "In this notebook, we integrate an mlp epinet with the regression layer. We'll try the simplest possible integration first, then perform training (which, given the small size, should be pretty quick), and iterate.\n",
    "\n",
    "How will we measure whether the integration works? First, we can sanity check by seeing how uncertainty compares across dimensions of rewards, comparing with our prior data on which dimensions have the most activation and which appear to be duplicates of each other. Ultimately, we can measure the performance of the reward model by doing some uncertainty-weighted best of N search.\n",
    "\n",
    "A note on form: prior versions of zetteldev had an emphasis on atomic notebooks for experimentation. We break from that. This document is more 'computational essay/lab report' than slip. It will contain many ideas, and confront much computational and ideological reducibility. The metaphors worthy of further abstraction will be highlighted in a separate report, so see the 'Reports' folder for the high level summary. What follows is a 'lab report' in chronological order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis**: \n",
    "1. Integrating the MLP reward model with an MLP epinet will enable the prediction of uncertainty per reward dimension per prompt.\n",
    "2. The uncertainty estimate should change with prompt response pairs.\n",
    "3. When the gating layer denotes a reward dimension as irrelevant, it should have a higher uncertainty.\n",
    "4. Integrating uncertainty into a Best of N sampler from a base llama model should have superior performance to the reward model without uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machinery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll set up the pretrained reward model, then extract dimensions and such from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, dataset_name = (\"FsfairX-LLaMA3-RM-v0.1\", \"ArmoRM-Multi-Objective-Data-v0.1\")\n",
    "save_dir = os.path.join(\"/home/piriac\", \"data\", \"ArmoRM\", \"regression_weights\")\n",
    "save_path = os.path.join(save_dir, f\"{model_name}_{dataset_name}.pt\")\n",
    "regression_layer = torch.load(save_path)[\"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_attributes, hidden_size = regression_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_rewards = torch.rand(800,hidden_size) @ regression_layer.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset prompt-response embeddings from the base LLM, as inputs to the regression layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from safetensors.torch import load_file\n",
    "from argparse import ArgumentParser\n",
    "def load_embeddings_and_preferences(embeddings_dir=None, model_name=None, dataset_name=None):\n",
    "    \"\"\"\n",
    "    Load embeddings and preferences from safetensors files.\n",
    "    \n",
    "    Args:\n",
    "        embeddings_dir (str, optional): Path to embeddings directory\n",
    "        model_name (str, optional): Name of the model\n",
    "        dataset_name (str, optional): Name of the dataset\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (embeddings tensor, labels tensor)\n",
    "    \"\"\"\n",
    "    # Set default paths if not provided\n",
    "    HOME = os.path.expanduser(\"~\")\n",
    "    if embeddings_dir is None:\n",
    "        embeddings_dir = os.path.join(\n",
    "            HOME, \"data\", \"ArmoRM\", \"embeddings\", model_name, dataset_name\n",
    "        )\n",
    "\n",
    "    # Collect all embedding files\n",
    "    embedding_files = sorted(glob(f\"{embeddings_dir}-*.safetensors\"))\n",
    "    \n",
    "    if not embedding_files:\n",
    "        raise FileNotFoundError(f\"No embedding files found in {embeddings_dir}\")\n",
    "\n",
    "    # Initialize lists to store embeddings and labels\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    print(\"Loading embeddings and labels from Safetensors files...\")\n",
    "    for file in tqdm(embedding_files, desc=\"Loading embeddings\"):\n",
    "        # Load the safetensors file\n",
    "        data = load_file(file)\n",
    "        embeddings.append(data[\"embeddings\"])  # Append embeddings tensor\n",
    "        labels.append(data[\"labels\"])  # Append labels tensor\n",
    "\n",
    "    # Concatenate all embeddings and labels into single tensors\n",
    "    embeddings = torch.cat(embeddings, dim=0).float()\n",
    "    labels = torch.cat(labels, dim=0).float()\n",
    "\n",
    "    print(f\"Total embeddings loaded: {embeddings.shape[0]}\")\n",
    "    print(f\"Total labels loaded: {labels.shape[0]}\")\n",
    "    \n",
    "    # Verify shapes match\n",
    "    assert embeddings.shape[0] == labels.shape[0], \"Number of embeddings and labels must match\"\n",
    "    \n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings and labels from Safetensors files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading embeddings: 100%|██████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1529.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings loaded: 569185\n",
      "Total labels loaded: 569185\n"
     ]
    }
   ],
   "source": [
    "embeddings, sparse_rewards = load_embeddings_and_preferences(\n",
    "    model_name=\"FsfairX-LLaMA3-RM-v0.1\",\n",
    "    dataset_name=\"ArmoRM-Multi-Objective-Data-v0.1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 15% of the reward labels are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([569185, 19])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preferences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1647670])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preferences[preferences == preferences].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15235727168532293"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1647670/(569185*19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4844,  1.1484,  1.0781,  ..., -0.5156, -0.5703,  1.9453],\n",
       "        [-1.4453,  1.3984,  0.5352,  ..., -0.4980, -0.8047,  1.6328],\n",
       "        [-1.5156,  1.7812,  1.5781,  ...,  0.1514, -0.6133,  2.3438],\n",
       "        ...,\n",
       "        [-1.4141,  1.5781,  1.1484,  ..., -0.9023, -0.9961,  1.7500],\n",
       "        [-1.3906,  1.7422,  1.2969,  ..., -0.6797, -0.4570,  2.0000],\n",
       "        [-1.4531,  1.8125,  1.3047,  ..., -0.9688, -0.3887,  2.2344]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check the regression weights by seeing how well it matches the preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_rewards = embeddings @ regression_layer.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~torch.isnan(sparse_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diff = (sparse_rewards - predicted_rewards)[~torch.isnan(sparse_rewards)].numpy()\n",
    "mse_diff = np.mean(diff**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022128979"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the reward prediction is extremely successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the epinet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first epinet, we'll use a two layer mlp for the randomized component, and a pure linear layer for deterministic component. Thus, the non-randomized network recreates the Ridge regression setting. The epinet is given slightly more structure under the intuition that it needs not only to reproduce the computations of the deterministic component, but also reason about when those calculations need added randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:murdo.epinet_mlp:Creating MLPEpinet with output sizes: [4096, 19], epinet hiddens: [4096, 512], index dim: 8\n"
     ]
    }
   ],
   "source": [
    "from murdo.epinet_mlp import make_mlp_epinet\n",
    "epinet, indexer = make_mlp_epinet(\n",
    "    output_sizes = [hidden_size,n_attributes],\n",
    "    epinet_hiddens = [hidden_size, 512],\n",
    "    index_dim = 8,\n",
    "    prior_scale = 1, \n",
    "    name = \"my first epinet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPEpinet(\n",
       "  (base_mlp): ExposedMLP(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=4096, out_features=19, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (train_epinet): ProjectedMLP(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=152, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (prior_epinet): ProjectedMLP(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=152, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epinet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:murdo.epinet_mlp:GaussianIndexer generating index with dimension 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 3.0516, -1.0326,  0.9805,  1.9015, -1.8265, -1.1090,  0.3039, -0.2242,\n",
       "         0.2080, -0.2025, -0.3878, -0.6146,  0.8108,  0.5130,  0.3731, -0.5977,\n",
       "        -0.1985, -0.6946, -2.5827])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we'll follow the same principle as is the paper: simply masking the unknown dimensions when calculating losses. This is hopefully sufficiently in keeping with the nature of SGD. Future work might explore using uncertainty to more cleverly compensate for missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rmurdo-zetteldev",
   "language": "python",
   "name": "rmurdo-zetteldev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "## Standard libraries\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "from fastcore.all import *\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "# set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "## project specifics\n",
    "import murdo\n",
    "import transformers\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating Epinet Into Armo Regression\n",
    "> C'mon, c'mon and meet the epinets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArmoRM's regression layer is pretty simple: it's just a linear layer, with a bias term, optimized through Ridge regression. In other words, it's just performing a change of basis on the underlying last-layer embeddings of Llama3 8b. And apparently, dimensions already exist in this marvelous latent space that correspond to many of the dimensions we care about.\n",
    "\n",
    "What can the epinet add to this? First, the structure fo the epinet should be pretty simple: we'll replicate the linear layer being used with Ridge Regression an MLP (hoping gradient descent is sufficient to extract the same dimensional information; and assuming we need extra compute power to both get the dimensions and reason about their epistemic status), but will multiply the output by the random epistemic index. In theory, this should allow the epinet to reduce the MSE regression loss by adding randomness to dimensions with questionable epistemic status.\n",
    "\n",
    "In this notebook, we integrate an mlp epinet with the regression layer. We'll try the simplest possible integration first, then perform training (which, given the small size, should be pretty quick), and iterate.\n",
    "\n",
    "How will we measure whether the integration works? First, we can sanity check by seeing how uncertainty compares across dimensions of rewards, comparing with our prior data on which dimensions have the most activation and which appear to be duplicates of each other. Ultimately, we can measure the performance of the reward model by doing some uncertainty-weighted best of N search.\n",
    "\n",
    "A note on form: prior versions of zetteldev had an emphasis on atomic notebooks for experimentation. We break from that. This document is more 'computational essay/lab report' than slip. It will contain many ideas, and confront much computational and ideological reducibility. The metaphors worthy of further abstraction will be highlighted in a separate report, so see the 'Reports' folder for the high level summary. What follows is a 'lab report' in chronological order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis**:\n",
    "1. Integrating the MLP reward model with an MLP epinet will enable the prediction of uncertainty per reward dimension per prompt.\n",
    "2. The uncertainty estimate should change with prompt response pairs.\n",
    "3. When the gating layer denotes a reward dimension as irrelevant, it should have a higher uncertainty.\n",
    "4. Integrating uncertainty into a Best of N sampler from a base llama model should have superior performance to the reward model without uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machinery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll set up the pretrained reward model, then extract dimensions and such from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, dataset_name = (\"FsfairX-LLaMA3-RM-v0.1\", \"ArmoRM-Multi-Objective-Data-v0.1\")\n",
    "save_dir = os.path.join(\"/home/piriac\", \"data\", \"ArmoRM\", \"regression_weights\")\n",
    "save_path = os.path.join(save_dir, f\"{model_name}_{dataset_name}.pt\")\n",
    "regression_layer = torch.load(save_path)[\"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_attributes, hidden_size = regression_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_rewards = torch.rand(800,hidden_size) @ regression_layer.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset prompt-response embeddings from the base LLM, as inputs to the regression layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from safetensors.torch import load_file\n",
    "from argparse import ArgumentParser\n",
    "def load_embeddings_and_preferences(embeddings_dir=None, model_name=None, dataset_name=None):\n",
    "    \"\"\"\n",
    "    Load embeddings and preferences from safetensors files.\n",
    "\n",
    "    Args:\n",
    "        embeddings_dir (str, optional): Path to embeddings directory\n",
    "        model_name (str, optional): Name of the model\n",
    "        dataset_name (str, optional): Name of the dataset\n",
    "\n",
    "    Returns:\n",
    "        tuple: (embeddings tensor, labels tensor)\n",
    "    \"\"\"\n",
    "    # Set default paths if not provided\n",
    "    HOME = os.path.expanduser(\"~\")\n",
    "    if embeddings_dir is None:\n",
    "        embeddings_dir = os.path.join(\n",
    "            HOME, \"data\", \"ArmoRM\", \"embeddings\", model_name, dataset_name\n",
    "        )\n",
    "\n",
    "    # Collect all embedding files\n",
    "    embedding_files = sorted(glob(f\"{embeddings_dir}-*.safetensors\"))\n",
    "\n",
    "    if not embedding_files:\n",
    "        raise FileNotFoundError(f\"No embedding files found in {embeddings_dir}\")\n",
    "\n",
    "    # Initialize lists to store embeddings and labels\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    print(\"Loading embeddings and labels from Safetensors files...\")\n",
    "    for file in tqdm(embedding_files, desc=\"Loading embeddings\"):\n",
    "        # Load the safetensors file\n",
    "        data = load_file(file)\n",
    "        embeddings.append(data[\"embeddings\"])  # Append embeddings tensor\n",
    "        labels.append(data[\"labels\"])  # Append labels tensor\n",
    "\n",
    "    # Concatenate all embeddings and labels into single tensors\n",
    "    embeddings = torch.cat(embeddings, dim=0).float()\n",
    "    labels = torch.cat(labels, dim=0).float()\n",
    "\n",
    "    print(f\"Total embeddings loaded: {embeddings.shape[0]}\")\n",
    "    print(f\"Total labels loaded: {labels.shape[0]}\")\n",
    "\n",
    "    # Verify shapes match\n",
    "    assert embeddings.shape[0] == labels.shape[0], \"Number of embeddings and labels must match\"\n",
    "\n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings and labels from Safetensors files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading embeddings: 100%|██████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1548.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings loaded: 569185\n",
      "Total labels loaded: 569185\n"
     ]
    }
   ],
   "source": [
    "embeddings, sparse_rewards = load_embeddings_and_preferences(\n",
    "    model_name=\"FsfairX-LLaMA3-RM-v0.1\",\n",
    "    dataset_name=\"ArmoRM-Multi-Objective-Data-v0.1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 15% of the reward labels are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([569185, 19])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_rewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1647670])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_rewards[sparse_rewards == sparse_rewards].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15235727168532293"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1647670/(569185*19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4844,  1.1484,  1.0781,  ..., -0.5156, -0.5703,  1.9453],\n",
       "        [-1.4453,  1.3984,  0.5352,  ..., -0.4980, -0.8047,  1.6328],\n",
       "        [-1.5156,  1.7812,  1.5781,  ...,  0.1514, -0.6133,  2.3438],\n",
       "        ...,\n",
       "        [-1.4141,  1.5781,  1.1484,  ..., -0.9023, -0.9961,  1.7500],\n",
       "        [-1.3906,  1.7422,  1.2969,  ..., -0.6797, -0.4570,  2.0000],\n",
       "        [-1.4531,  1.8125,  1.3047,  ..., -0.9688, -0.3887,  2.2344]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check the regression weights by seeing how well it matches the preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_rewards = embeddings @ regression_layer.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~torch.isnan(sparse_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diff = (sparse_rewards - predicted_rewards)[~torch.isnan(sparse_rewards)].numpy()\n",
    "mse_diff = np.mean(diff**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022128979"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the reward prediction is extremely successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the epinet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first epinet, we'll use a two layer mlp for the randomized component, and a pure linear layer for deterministic component. Thus, the non-randomized network recreates the Ridge regression setting. The epinet is given slightly more structure under the intuition that it needs not only to reproduce the computations of the deterministic component, but also reason about when those calculations need added randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:murdo.epinet_mlp:Creating MLPEpinet with output sizes: [10, 64, 32, 2], epinet hiddens: [10, 8], index dim: 8\n",
      "INFO:murdo.epinet_mlp:Creating MLPEpinet with output sizes: [4096, 19], epinet hiddens: [4115, 512], index dim: 8\n"
     ]
    }
   ],
   "source": [
    "from murdo.epinet_mlp import make_mlp_epinet\n",
    "epinet, indexer = make_mlp_epinet(\n",
    "    output_sizes = [hidden_size,n_attributes],\n",
    "    epinet_hiddens = [hidden_size + n_attributes, 512],\n",
    "    index_dim = 8,\n",
    "    prior_scale = 1,\n",
    "    name = \"my first epinet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPEpinet(\n",
       "  (base_mlp): ExposedMLP(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=4096, out_features=19, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (train_epinet): ProjectedMLP(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=4115, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=152, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (prior_epinet): ProjectedMLP(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=4115, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=152, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epinet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:murdo.epinet_mlp:GaussianIndexer generating index with dimension 8\n",
      "INFO:murdo.epinet_mlp:my first epinet forward pass - Input shape: torch.Size([64, 4096]), Index shape: torch.Size([8])\n",
      "INFO:murdo.epinet_mlp:my first epinet_base_mlp forward pass - Input shape: torch.Size([64, 4096])\n",
      "INFO:murdo.epinet_mlp:my first epinet_base_mlp layer 0 output shape: torch.Size([64, 19])\n",
      "INFO:murdo.epinet_mlp:my first epinet_base_mlp final output shape: torch.Size([64, 19])\n",
      "INFO:murdo.epinet_mlp:my first epinet_base_mlp exposed features shape: torch.Size([64, 4115])\n",
      "INFO:murdo.epinet_mlp:my first epinet base network features shape: torch.Size([64, 4115])\n",
      "INFO:murdo.epinet_mlp:my first epinet_train_epinet forward pass - Input shape: torch.Size([64, 4115]), Index shape: torch.Size([8])\n",
      "INFO:murdo.epinet_mlp:my first epinet_train_epinet MLP output shape: torch.Size([64, 152])\n",
      "INFO:murdo.epinet_mlp:my first epinet_train_epinet reshaped output shape: torch.Size([64, 19, 8])\n",
      "INFO:murdo.epinet_mlp:my first epinet_train_epinet final output shape: torch.Size([64, 19])\n",
      "INFO:murdo.epinet_mlp:my first epinet_prior_epinet forward pass - Input shape: torch.Size([64, 4115]), Index shape: torch.Size([8])\n",
      "INFO:murdo.epinet_mlp:my first epinet_prior_epinet MLP output shape: torch.Size([64, 152])\n",
      "INFO:murdo.epinet_mlp:my first epinet_prior_epinet reshaped output shape: torch.Size([64, 19, 8])\n",
      "INFO:murdo.epinet_mlp:my first epinet_prior_epinet final output shape: torch.Size([64, 19])\n",
      "INFO:murdo.epinet_mlp:my first epinet final output shapes - Train: torch.Size([64, 19]), Prior: torch.Size([64, 19])\n"
     ]
    }
   ],
   "source": [
    "# example usage\n",
    "output = epinet(torch.randn(64, hidden_size), indexer(64))\n",
    "train_predictions = output.train\n",
    "prior_predictions = output.prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 19])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:murdo.epinet_mlp:GaussianIndexer generating index with dimension 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2318,  0.5307, -0.5648,  0.0082, -0.8816, -1.0836, -2.2597,  0.4693])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we'll follow the same principle as is the paper: simply masking the unknown dimensions when calculating losses. This is hopefully sufficiently in keeping with the nature of SGD. Future work might explore using uncertainty to more cleverly compensate for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, trange\n",
    "def train_epinet(epinet, indexer, embeddings, sparse_rewards, hidden_size,\n",
    "                 batch_size=64, num_epochs=100, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train the epinet using masked MSE loss.\n",
    "\n",
    "    Args:\n",
    "        epinet: The epinet MLP model\n",
    "        indexer: The indexer function for epinet\n",
    "        embeddings: Input embeddings tensor\n",
    "        sparse_rewards: Sparse reward labels tensor\n",
    "        hidden_size: Size of the hidden dimension\n",
    "        batch_size: Batch size for training\n",
    "        num_epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    epinet = epinet.to(device)\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = optim.Adam(epinet.parameters(), lr=lr)\n",
    "\n",
    "    # Calculate number of batches\n",
    "    n_samples = embeddings.shape[0]\n",
    "    n_batches = n_samples // batch_size\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Shuffle data\n",
    "        perm = torch.randperm(n_samples)\n",
    "        embeddings = embeddings[perm]\n",
    "        sparse_rewards = sparse_rewards[perm]\n",
    "\n",
    "        # Batch training\n",
    "        pbar = tqdm(range(n_batches), desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for b in pbar:\n",
    "            # Get batch and move to device\n",
    "            start_idx = b * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_embeddings = embeddings[start_idx:end_idx].to(device)\n",
    "            batch_rewards = sparse_rewards[start_idx:end_idx].to(device)\n",
    "\n",
    "            # Generate random indices for epinet\n",
    "            indices = indexer(batch_size).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            epiout = epinet(batch_embeddings, indices)\n",
    "            predicted_rewards = epiout.train + epiout.prior # preweighted sum of the learnable and fixed components \n",
    "            \n",
    "            # Create mask for non-nan values\n",
    "            mask = ~torch.isnan(batch_rewards)\n",
    "\n",
    "            # Calculate masked MSE loss\n",
    "            loss = torch.mean((predicted_rewards[mask] - batch_rewards[mask])**2)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': epoch_loss/(b+1)})\n",
    "\n",
    "            # Free memory\n",
    "            del batch_embeddings\n",
    "            del batch_rewards\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Print epoch statistics\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {epoch_loss/n_batches:.6f}')\n",
    "\n",
    "    return epinet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train! From above, Ridge regression without the epistemic nn achieved a loss of 0.022. Let's see if we can match that in the same order of magnitude, and perhaps even get below it.\n",
    "Although as we've also changed the methodology (to gradient descent), and are adding randomness to the outputs, the raw numbers aren't directly comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below, we can quickly get in the same order of magnitude; the remainder of training is how to exploit randomness efficiently to further minimize the loss. It will be a good sanity check to see how much randomness is added: is the plain MLP doing most of the work, or are outputs of a substantial magnitude coming from the randomized portion of the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|███████████████████████████████████████████████████████████| 8893/8893 [00:14<00:00, 624.58it/s, loss=0.0405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Average Loss: 0.040519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|███████████████████████████████████████████████████████████| 8893/8893 [00:14<00:00, 619.65it/s, loss=0.0316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Average Loss: 0.031624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|███████████████████████████████████████████████████████████| 8893/8893 [00:13<00:00, 639.37it/s, loss=0.0297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Average Loss: 0.029733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|███████████████████████████████████████████████████████████| 8893/8893 [00:14<00:00, 626.99it/s, loss=0.0292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Average Loss: 0.029166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|███████████████████████████████████████████████████████████| 8893/8893 [00:14<00:00, 632.70it/s, loss=0.0281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Average Loss: 0.028134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|███████████████████████████████████████████████████████████| 8893/8893 [00:13<00:00, 640.71it/s, loss=0.0276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Average Loss: 0.027649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|███████████████████████████████████████████████████████████| 8893/8893 [00:13<00:00, 639.87it/s, loss=0.0272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Average Loss: 0.027246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|███████████████████████████████████████████████████████████| 8893/8893 [00:13<00:00, 640.03it/s, loss=0.0269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Average Loss: 0.026911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|███████████████████████████████████████████████████████████| 8893/8893 [00:13<00:00, 642.98it/s, loss=0.0264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Average Loss: 0.026403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100:  45%|██████████████████████████▏                               | 4011/8893 [00:06<00:07, 616.30it/s, loss=0.0247]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 27/100:  76%|███████████████████████████████████████████▉              | 6744/8893 [00:10<00:03, 630.62it/s, loss=0.0247]"
     ]
    }
   ],
   "source": [
    "trained_epinet = train_epinet(\n",
    "    epinet=epinet,\n",
    "    indexer=indexer,\n",
    "    embeddings=embeddings,\n",
    "    sparse_rewards=sparse_rewards,\n",
    "    hidden_size=hidden_size,\n",
    "    lr = 1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rmurdo-zetteldev",
   "language": "python",
   "name": "rmurdo-zetteldev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
